{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Federated Learning Problem Set\n",
    "\n",
    "Through the course of this problem set, you will examine the condition under which a worker chooses to upload its gradient weights in the LASG-WK2 algorithm. Then, you will perform a hyperparameter search to compare LASG-WK2 and the original Federated Averaging algorithm, and comment on the results. You will need to understand the hyperparameters being used and how they affect the update condition in order to effectively accomplish this. Once finished, you should have a more thorough understanding of LASG-WK2 and how it translates to communication/computation benefits in the real world.\n",
    "\n",
    "This problem set references the \"LASG: Lazily Aggregated Stochastic Gradients for Communication-Efficient Distributed Learning\" paper by Chen et al, available at https://arxiv.org/pdf/2002.11360.pdf. This paper will colloquially be addressed as \"the LASG paper\" or equivalent throughout.\n",
    "\n",
    "First, we will import a different dataset from the one we used in our experiments. For simplicity, let's use fashion_mnist since the preprocessing to be done is the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# module imports\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# imports from necessary files in our github repo\n",
    "import runner\n",
    "import lasg\n",
    "import federated_avg\n",
    "\n",
    "#Load the data\n",
    "(X_train, y_train), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "#Scale the images\n",
    "X_train = X_train.astype(\"float32\") / 255\n",
    "X_test = X_test.astype(\"float32\") / 255\n",
    "\n",
    "#Add black/white channel dimension\n",
    "X_train = np.expand_dims(X_train, -1)\n",
    "X_test = np.expand_dims(X_test, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for your first task. Write a function which implements the LASG-WK2 update condition as detailed in equation 10 of the LASG paper. Follow the prototype detailed below. You will need to understand each of the terms being referenced in equation 10, which include some of the hyperparameters. Note that the function below takes as inputs some pre-computed terms - you will have to understand what these are as well.\n",
    "\n",
    "For simplicity, you may wish to follow the recommendation of the authors in the LASG paper and take c as a constant for only the ten most recent historical values, and as zero for all older values. This will slightly simplify your implementation. However, you are welcome to play around with different weights, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lasg_wk2_update(current_weight_grad, last_upload_weight_grad, weights_diff_history, M, c):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        current_weight_grad - the gradient of the worker's current weights\n",
    "        last_upload_weight_grad - the gradient of the last set of weights the worker uploaded\n",
    "        weights_diff_history - a vector of length D containing the squared norm of this worker's weight \n",
    "                               differences in successive iterations, for the last D iterations.\n",
    "        M - the total number of workers in the federated learning system.\n",
    "        c -  a vector of length D containing the weight(s) that should be applied to each historical \n",
    "             value of weights_diff_history. \n",
    "    \n",
    "    Outputs:\n",
    "        Either true or false depending whether the update condition was met.\n",
    "        If true, this means that the worker checking this condition should not upload (and we have saved on communication! yay!)\n",
    "        If false, this means that the worker should upload its model.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice job! You did it, and now you have hopefully gained an understanding of what should be considered when deciding whether a round of communication is worthwhile, as well as how some of the hyperparameters (number of workers and weight vector C) affect the LASG algorithm. You may check your code against our implementation in our lasg.py file.\n",
    "\n",
    "Now for something a little more interesting. In practice, we observe that Federated Averaging beats out LASG in most cases. Perform a hyperparameter search to find a case (or multiple cases) wherein LASG beats out Federated Averaging (e.g. reaches the same accuracy with fewer rounds of communication). Comment on the case(s) where this occurs - where might this occur in the real world? Why is Federated Learning useful in such a scenario?\n",
    "\n",
    "Below are some brief descriptions of each hyperparameter, so that you can understand what you are changing. We recommend you look at the LASG paper if you wish to see more details about how these hyperparameters affect the LASG-WK2 algorithm.\n",
    "\n",
    "M : int - The number of workers. The default is 10.\n",
    "\n",
    "K : int - Maximum number of iterations of the LASG algorithm to run. The default is 10000.\n",
    "\n",
    "D : int - The maximum number of LASG iterations that a worker's stale gradient can be reused. The default is 50.\n",
    "\n",
    "c : float - Scalar weight used in the right-hand side of the LASG-WK2 update condition. The default is computed based on eta and M, as specified in the paper.\n",
    "\n",
    "c_range : int - The number of most recent updates to apply c to in the LASG-WK2 update condition. Any beyond this value will receive weights of 0. The default is 10.\n",
    "\n",
    "eta : float - Step size to use for worker SGD optimizers. The default is 0.05.\n",
    "\n",
    "B : int or float - The worker minibatch size (integer > 0 or fraction of the worker local dataset size if B_is_fraction=True). The default is 100.\n",
    "\n",
    "B_is_fraction : boolean - see B above. The default is False.\n",
    "\n",
    "iid : boolean - If iid, the data is randomly and evenly distributed among the workers. If not iid, each worker gets data in only one (or few) classes, which potentially is more representative of a real-world scenario.\n",
    "\n",
    "evaluation_interval : int - Evaluate global model whenever this many iterations of LASG have been run. The default is 5.\n",
    "\n",
    "target_val_accuracy : float - Stop training if this target validation accuracy has been achieved. The default is 0.95.\n",
    "\n",
    "print_lasg_wk2_condition : boolean - If true print the LASG-WK2 condition (equation 10 in the paper) for each worker at each iteration. The default is False. This does not change anything about the algorithm, but rather allows you to see how frequently workers are updating/communicating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model, loss, and optimizer\n",
    "# This mirrors the setup used in the LASG paper, do not change anything here.\n",
    "def model_constructor(hparams):\n",
    "    model = keras.Sequential([\n",
    "            keras.Input(shape=X_train.shape[1:]),\n",
    "            layers.Conv2D(20, kernel_size=(5, 5), activation=\"elu\"),\n",
    "            layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "            layers.Conv2D(50, kernel_size=(5, 5), activation=\"elu\"),\n",
    "            layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "            layers.Flatten(),\n",
    "            layers.Dense(500, activation=\"elu\"),\n",
    "            layers.Dense(10, activation=\"linear\")\n",
    "        ])\n",
    "\n",
    "    sgd_optimizer = keras.optimizers.SGD(learning_rate=hparams.eta)\n",
    "    model.compile(loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True), \n",
    "                  optimizer=sgd_optimizer, metrics=[\"accuracy\"])\n",
    "\n",
    "    return model\n",
    "\n",
    "# Decide on a test accuracy to aim for. If your tests are taking too long, you may lower this, \n",
    "# since the goal is to see which method reaches a target accuracy with the least communication, rather than to \n",
    "# achieve a higher accuracy with a certain number of iterations.\n",
    "target_test_accuracy = 0.95\n",
    "\n",
    "# Set the hyperparameters for Federated Averaging\n",
    "# Leave these untouched as a baseline - these are the default values from the original federated averaging paper.\n",
    "fedavg_hparams = federated_avg.fedavg_hparams(K=100, C=0.1, E=1, B=10, eta=0.1, MAX_T=10000, evaluation_interval=2, \n",
    "                                              target_val_accuracy=target_test_accuracy)\n",
    "\n",
    "# Set the hyperparameters for LASG-WK2\n",
    "# These are what you will primarily be changing.\n",
    "lasg_wk2_hparams = lasg.lasg_wk2_hparams(M=10, K=10000, D=50, c=4000, c_range=10, eta=0.1, B=100, \n",
    "                                         B_is_fraction=False,\n",
    "                                         iid=True,\n",
    "                                         evaluation_interval=5, \n",
    "                                         target_val_accuracy=target_test_accuracy, \n",
    "                                         print_lasg_wk2_condition=False)\n",
    "\n",
    "# Use the runner helper function to run the experiments\n",
    "# Do not edit this function.\n",
    "_, fedavg_log, lasg_wk2_log = runner.run_experiments(\n",
    "                    X_train, y_train, X_test, y_test, model_constructor, \n",
    "                    vanilla_sgd_hparams=None, fedavg_hparams, lasg_wk2_hparams, seed=100)\n",
    "\n",
    "# Code for plotting your results\n",
    "plt.plot(fedavg_log[\"communication_rounds\"], fedavg_log[\"loss\"], label=\"FederatedAveraging\")\n",
    "plt.plot(lasg_wk2_log[\"communication_rounds\"], lasg_wk2_log[\"loss\"], label=\"LASG-WK2\")\n",
    "plt.set_xlabel(\"Communication (rounds of upload)\")\n",
    "plt.set_ylabel(\"Loss\")\n",
    "plt.set_title(\"FederatedAveraging vs. LASG-WK2 \\n on MNIST CNN ({0})\".format(iid_label))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(For CSCI-6961 Students Only) - Implement the LASG-WK1 Algorithm, detailed in Table 2, Algorithm 1 of the LASG paper. The prototype and docstring have been provided. This will require using a different update condition (equation (8) in the LASG paper). You may reference the code we wrote for LASG-WK2 in the lasg_wk2() function in the lasg.py file in the github repository [insert link to that file here]. You will need to make some slight changes, since the algorithms are different.\n",
    "\n",
    "Then, compare the performance of LASG-WK1 and LASG-WK2 on the same dataset with the same hyperparameters. Based on your results, why might you prefer one algorithm over the over?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lasg_wk1(X_train, y_train, X_val, y_val, model_constructor, hparams, rng=None):\n",
    "    \"\"\"\n",
    "    Simulate training a model using LASG-WK1 across M distributed devices.\n",
    "    Return the final global model and metrics gathered over the course of the run.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train : numpy ndarray\n",
    "        Training features.\n",
    "    y_train : numpy ndarray\n",
    "        Training targets.\n",
    "    X_val : numpy ndarray\n",
    "        Validation features.\n",
    "    y_val : numpy ndarray\n",
    "        Validation targets.\n",
    "    model_constructor : function\n",
    "        function that constructs a compiled tf.keras.Model using hparams.\n",
    "    hparams : lasg_wk1_hparams\n",
    "        Hyperparameters for LASG-WK1. Note that these may be generated using the lasg_wk2_hparams.\n",
    "        Despite the name difference, the two use the same hyperparameters.\n",
    "    rng : numpy.random.Generator, optional\n",
    "       instance to use for random number generation.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    global_model : tf.keras.Model\n",
    "        The final global model\n",
    "    \n",
    "    log : dict\n",
    "        Dictionary containing training and validation metrics:\n",
    "            loss : \n",
    "                training loss at each iteration\n",
    "            accuracy : \n",
    "                training accuracy at each iteration\n",
    "            val_loss : \n",
    "                validation loss at each iteration\n",
    "            val_accuracy : \n",
    "                validation accuracy at each iteration\n",
    "            iteration : \n",
    "                the iteration number at which the measurements were made\n",
    "            communication_rounds : \n",
    "                the cumulative number of worker uploads by each iteration\n",
    "            worker_upload_fraction : \n",
    "                the average fraction of workers who upload each iteration\n",
    "    \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
